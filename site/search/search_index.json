{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to MkDocs","text":"<p>For full documentation visit mkdocs.org.</p>"},{"location":"#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"accessssh/","title":"Access via SSH and Scripts","text":"<p>This option is for more advanced researchers with relatively solid Python (or similar) skills who need to query the database by their scripts and the web downloader is not powerful enough for their project requirements. Undergraduate students are strongly advised to try the downloader first. </p> <ol> <li> <p>Generate a new ssh key pair on the computer you want to access the database from (see Appendix 1 and 2). </p> </li> <li> <p>Send to Nargess (n.nourbakhsh@unsw.edu.au): </p> <ul> <li> <p>the contents of the \u201cpublic key\u201d (e.g. id_rsa.pub) </p> </li> <li> <p>the data you would like to have access to from below: </p> </li> </ul> <p>Tables of the Datasets: </p> <pre><code>- Ausgrid 300 Homes Solar Data\n\n- PV Bushfire Data\n\n- Solar Analytics Solar Data\n\n- Victorian Consumers Data\n\n- Energy Data Platform (by year/month)\n\n- BOM (by zone)\n</code></pre> <p>Data Tables from UNSW Buildings: </p> <pre><code>- photovoltaics\n\n- metadatapv\n\n- metadataunitpv\n\n- weather\n\n- metadataunitsweather\n\n- units\n\n- versioninfo\n</code></pre> </li> </ol> <p>Your access to the server and database will be granted accordingly and access details and list of data views will be emailed to you. You will be able to connect to the server and database using the access details and query the data views like any database table (SQL query, etc.). </p>"},{"location":"accessssh/#appendix-1-generating-a-new-ssh-key-pair-windows","title":"Appendix 1: Generating a New SSH Key Pair (Windows)","text":"<p>1.In a command (CMD) window or in a PowerShell window, run:  </p> <pre><code>*ssh-keygen*\n</code></pre> <ol> <li> <p>You will be prompted with a few questions: </p> <ul> <li> <p><code>Enter file in which to save the key (/home/&lt;user&gt;/.ssh/id_rsa):  &lt;you can just press enter here to accept the default* &gt;</code></p> </li> <li> <p><code>Enter passphrase (empty for no passphrase): &lt;Enter a phrase that you will be sure to remember as you\u2019ll need this each time you login**&gt;</code></p> </li> <li> <p><code>Enter same passphrase again: &lt;Enter the phrase again to verify&gt;</code></p> </li> </ul> <p>* For the file location, if you are prompted to overwrite an existing file (and you still need your previously existing file), do NOT do that as you will wipe out any other SSH keys you may have. Instead enter a different file name, but be sure to record the full pathname for use later when initiating the SSH session. </p> <p>** The passphrase can contain any characters including spaces. You should keep a record of it: without the passphrase you will not be able to login using SSH. </p> </li> <li> <p>If you used the default file location, you should see 2 new files in the <code>/home/&lt;user&gt;/.ssh</code> folder: </p> <ul> <li> <p>id_rsa </p> </li> <li> <p>id_rsa.pub </p> </li> </ul> <p>Open the id_rsa.pub file in a text editor and send the contents as plain text, as a .txt file, or just send the whole file. </p> <p>Note: the .pub file is your public key and can be shared without any security concerns, but the other file contains your private key which you should always keep secret. The 2 files work together to provide secure access just for you. </p> </li> </ol>"},{"location":"accessssh/#appendix-2-generating-a-new-ssh-key-pair-macos","title":"Appendix 2: Generating a New SSH Key Pair (MacOS)","text":"<ol> <li> <p>In Finder, choose Utilities from the Applications folder. </p> </li> <li> <p>Find Terminal in the Utilities list. </p> </li> <li> <p>Open Terminal. </p> </li> <li> <p>Enter the following command in the Terminal window: </p> <p>ssh-keygen -t rsa</p> </li> <li> <p>Enter key to accept the default location. </p> </li> <li> <p>Type in a passphrase (you will need to repeat the passphrase a second time to continue). </p> </li> <li> <p>You will see the random art created by SSH key-gen command. </p> </li> <li> <p>Your private key is saved to the id_rsa file in the .ssh directory and is used to verify the public key you use. </p> </li> <li> <p>Your public key is saved to the id_rsa.pub.</p> </li> <li> <p>You can copy your public key by the following command: </p> <p>pbcopy &lt; ~/.ssh/id_rsa.pub</p> </li> </ol>"},{"location":"accessweb/","title":"Access via Web Downloader","text":"<p>Original access to DARTH data has been through a web interface called SPREE Photovoltaics Data Downloader: https://darth.engineering.unsw.edu.au. It would suffice for many research purposes and most undergraduate projects. Any UNSW member with a valid zID can login and use this system to download the data: </p> <p></p> <p></p> <p>The first input field in the web interface is Data Type:</p> <p></p> <p>Each value in Data Type is in fact the name of a data table in the DARTH database. Each dataset in DARTH corresponds to one or more data tables. Selecting a value in this dropdown will narrow the download to the data from that table. </p> <p>Please note that the current implementation needs the Data Type, From Time Stamp, and To Time Stamp, to be selected before the download request can be processed. Like this: </p> <p></p> <p>Due to the large volume of some datasets, it is advisable to only select as small period as necessary/possible to avoid crashing the server or user\u2019s device, especially when trying to access the bigger datasets like EDP and BOM. If data from bigger time ranges are required, it is recommended to download data from smaller time ranges and combine them offline. </p> <p>When the Data Type and time filters are selected, the Download button will be activated (color will change from blue to yellow to indicate that). Clicking it will initiate the download request/process and the user will see a \u201cProcessing your request\u201d spinner: </p> <p></p> <p>After a while the data will be downloaded as a CSV file: </p> <p></p> <p>It is worth mentioning, especially with timestamped data, that the downloaded records are not always sorted in the expected way, i.e. not completely sorted by time. </p>"},{"location":"ausgrid300/","title":"Ausgrid 300 Homes Solar Data","text":"<p>This dataset consists of half-hourly solar generation data of 300 customers for 3 years (2010-2011, 2011-2012, 2012-2013). Two tables were created for this dataset: </p> <ol> <li> <p>ausgrid_solar_homes_customers:</p> <p>columns:</p> <ul> <li> <p>customer_id (integer; Not Null) </p> </li> <li> <p>postcode (character 4; Not Null) </p> </li> <li> <p>generator_capacity (double precision; Not Null) </p> </li> </ul> <p>Primary Key:</p> <ol> <li>ausgrid_solar_homes_elecrticity</li> </ol> <p>columns:</p> <ul> <li> <p>customer_id (integer; Not Null) </p> </li> <li> <p>datetime (timestamp without time zone; Not Null) </p> </li> <li> <p>date (date; Not Null) </p> </li> <li> <p>time (character varying 5; Not Null) </p> </li> <li> <p>gross_generation (real) </p> </li> <li> <p>general_consumption (real) </p> </li> <li> <p>controlled_load (real) </p> <p>Primary key: combination of customer_id and datetime </p> <p>Foreign key: customer_id referencing ausgrid_solar_homes_customers.customer_id</p> <p>The timestamp column of the processed dataset was renamed to datetime in the database table to activate the time filters of the webdownloader on this dataset. </p> </li> </ul> </li> </ol> <p>The total volume of this dataset is around 800 MB. </p>"},{"location":"ausgrid300/#appendix-a-original-dataset","title":"Appendix A: Original Dataset","text":"<p>The original dataset consisted of three csv files, each containing half-hourly solar generation data of 300 customers for one year (2010-2011, 2011-2012, 2012-2013). Each data row included customer id, generator capacity, postcode, consumption category, date, and 48 values. The csv files for the second and third year also included a column named Row Quality, but there were no data in it. The consumption category column could include three values: \u201cCL\u201d for controlled load, \u201cGC\u201d for general consumption, and \u201cGG\u201d for gross generation. Thus, in a complete dataset there would have been three data rows for each customer/date combination. However, in many cases there was no data for controlled load, that is only two rows existed for a specific customer/date combination. Therefore, each file had around 270,000 rows and 53 or 54 columns. Please refer to the Ausgrid solar home electricity data notes (Aug 2014).pdf for more details about the original dataset. </p>"},{"location":"bom/","title":"BOM (Bureau of Meteorology)","text":"<p>This dataset consists of half-hourly weather data from the Australian Bureau of Meteorology. It starts from 1985 and continues to the present and includes around 630 weather stations nation-wide. </p>"},{"location":"bom/#database-structure-data","title":"Database Structure &amp; Data","text":"<p>This dataset has two tables in the database: </p> Database Table Columns bom_station_details date_data_supplied, record_identifier, station_number, rainfall_district_code, station_name, month_year_station_opened, month_year_station_closed, latitude, longitude, method_by_which_latitude_longitude_was_derived, state, height_of_station_above_mean_sea_level_metres, height_of_barometer_above_mean_sea_level_metres, wmo_index_number, first_year_of_data_supplied, last_year_of_data_supplied, percentage_complete_between_first_and_last_records, percentage_of_values_with_quality_flag_y,  percentage_of_values_with_quality_flag_n, percentage_of_values_with_quality_flag_w, percentage_of_values_with_quality_flag_s, percentage_of_values_with_quality_flag_i, end_of_record_indicator bom_data hm, station_number, state, year, month, day, hour_local_std, min_local_std, datetime, precipitation_since_9am_local_time_in_mm, quality_of_precipitation_since_9am_local_time, air_temperature_in_degrees_c, quality_of_air_temperature, wet_bulb_temperature_in_degrees_c, quality_of_wet_bulb_temperature, dew_point_temperature_in_degrees_c, quality_of_dew_point_temperature, relative_humidity_in_percentage, quality_of_relative_humidity, vapour_pressure_in_hpa, quality_of_vapour_pressure, saturated_vapour_pressure_in_hpa, quality_of_saturated_vapour_pressure, wind_speed_in_km_h, wind_speed_quality, wind_direction_in_degrees_true, wind_direction_quality,  speed_of_maximum_windgust_in_last_10_minutes_in_km_h, quality_of_speed_of_maximum_windgust_in_last_10_minutes, cloud_amount_of_first_group_in_eighths, quality_of_first_group_of_cloud_amount, cloud_type_of_first_group_in_code, quality_of_first_group_of_cloud_type, cloud_height_of_first_group_in_feet, quality_of_first_group_of_cloud_height, cloud_amountof_second_group_in_eighths, quality_of_second_group_of_cloud_amount, cloud_type_of_second_group_in_code, quality_of_second_group_of_cloud_type, cloud_height_of_second_group_in_feet, quality_of_second_group_of_cloud_height, cloud_amount_of_third_group_in_eighths, quality_of_third_group_of_cloud_amount, cloud_type_of_third_group_in_code, quality_of_third_group_of_cloud_type, cloud_height_of_third_group_in_feet, quality_of_third_group_of_cloud_height, cloud_amount_of_fourth_group_in_eighths, quality_of_fourth_group_of_cloud_amount, cloud_type_of_fourth_group_in_code, quality_of_fourth_group_of_cloud_type, cloud_height_of_fourth_group_in_feet, quality_of_fourth_group_of_cloud_height, ceilometer_cloud_amount_of_first_group, quality_of_first_group_of_ceilometer_cloud_amount, ceilometer_cloud_height_of_first_group_in_feet, quality_of_first_group_of_ceilometer_cloud_height, ceilometer_cloud_amount_of_second_group, quality_of_second_group_of_ceilometer_cloud_amount, ceilometer_cloud_height_of_second_group_in_feet, quality_of_second_group_of_ceilometer_cloud_height, ceilometer_cloud_amount_of_third_group, quality_of_third_group_of_ceilometer_cloud_amount, ceilometer_cloud_height_of_third_group_in_feet, quality_of_third_group_of_ceilometer_cloud_height, ceilometer_sky_clear_flag, mean_sea_level_pressure_in_hpa, quality_of_mean_sea_level_pressure, station_level_pressure_in_hpa, quality_of_station_level_pressure, aws_flag, end_of_record_indicator <p>The station details are usually the same from month to month for each station, occasionally change for example a station is closed. Therefore, we keep all records of these details to have the details corresponding to the data in any specific date. However, it also means that the details are repeated in the table. </p> <p>An example of the bom_station_details for one month can be found here. </p> <p>Due to the high volume of the data, the bom_data table has been partitioned by zones (states): </p> <ul> <li> <p>bom_data_ant </p> </li> <li> <p>bom_data_nsw </p> </li> <li> <p>bom_data_nt </p> </li> <li> <p>bom_data_qld </p> </li> <li> <p>bom_data_sa </p> </li> <li> <p>bom_data_tas </p> </li> <li> <p>bom_data_vic </p> </li> <li> <p>bom_data_wa </p> </li> </ul> <p>Each of the above partitions are accessible via the web downloader by selecting the name in the Data Type input field. It is recommended to use the Time From and Time To filters in the web interface to limit the data download volume to the necessary amount. Currently the size of this dataset is around 35 GB, each zone around 5 GB (on average). </p>"},{"location":"bom/#appendix-a-original-dataset","title":"Appendix A: Original Dataset","text":"<p>At the beginning of the project, we received the historical data up to May 2022 in a batch. It consisted of two metadata files (station details and station notes) for all the stations, plus one file of historical weather data for each station. Afterwards, we have been receiving monthly data updates including one monthly data file for each weather station, and two metadata files for all the stations.  </p>"},{"location":"bom/#1-notes","title":"1. Notes","text":"<p>With each data round one Notes metadata file is provided which explains the period of observation, byte location, byte size, and explanation of the fields in the data files, parameter notes, code meanings, and finally byte location, byte size, and explanation of the fields in the station details file. An example of the Notes file can be found here. </p>"},{"location":"bom/#2-station-details","title":"2. Station Details","text":"<p>With each data round one Station Details metadata file is provided as a text file. The file starts with the following explanation/disclaimer: </p> <pre><code>\u201cPercentage complete between first and last records assumes 48 observation per day.\n\nSome days there may be more than 48 reports if there have been significant changes in\n\nthe weather, so the percentage completeness is an estimate.\u201d\n</code></pre> <p>Then it has around 630 rows of details, one row for each weather station, including the following fields: </p> <ul> <li> <p>Record identifier - st </p> </li> <li> <p>Bureau of Meteorology Station Number </p> </li> <li> <p>Rainfall district code </p> </li> <li> <p>Station Name </p> </li> <li> <p>Month/Year station opened (MM/YYYY) </p> </li> <li> <p>Month/Year station closed if applicable (MM/YYYY) </p> </li> <li> <p>Latitude to 4 decimal places, in decimal degrees </p> </li> <li> <p>Longitude to 4 decimal places, in decimal degrees </p> </li> <li> <p>Method by which latitude/longitude was derived </p> </li> <li> <p>State </p> </li> <li> <p>Height of station above mean sea level in metres </p> </li> <li> <p>Height of barometer above mean sea level in metres </p> </li> <li> <p>WMO (World Meteorological Organisation) Index Number </p> </li> <li> <p>First year of data supplied in data file </p> </li> <li> <p>Last year of data supplied in data file </p> </li> <li> <p>Percentage complete between first and last records </p> </li> <li> <p>Percentage of values with quality flag 'Y' </p> </li> <li> <p>Percentage of values with quality flag 'N' </p> </li> <li> <p>Percentage of values with quality flag 'W' </p> </li> <li> <p>Percentage of values with quality flag 'S' </p> </li> <li> <p>Percentage of values with quality flag 'I' </p> </li> <li> <p># symbol, end of record indicator </p> </li> </ul>"},{"location":"bom/#3-data-files","title":"3. Data Files","text":"<p>In each round we receive a text data file for each weather station (around 630 data files). Each file has a header of the column names: </p> <pre><code>hm, Station Number, Year Month Day Hour Minutes in YYYY, MM, DD, HH24, MI format in Local standard time, Precipitation since 9am local time in mm, Quality of precipitation since 9am local time, Air Temperature in degrees C, Quality of air temperature, Wet bulb temperature in degrees C, Quality of Wet bulb temperature, Dew point temperature in degrees C, Quality of dew point temperature, Relative humidity in percentage %, Quality of relative humidity, Vapour pressure in hPa, Quality of vapour pressure, Saturated vapour pressure in hPa, Quality of saturated vapour pressure, Wind speed in km/h, Wind speed quality, Wind direction in degrees true, Wind direction quality, Speed of maximum windgust in last 10 minutes in  km/h, Quality of speed of maximum windgust in last 10 minutes, Cloud amount (of first group) in eighths, Quality of first group of cloud amount, Cloud type (of first group) in code, Quality of first group of cloud type, Cloud height (of first group) in feet, Quality of first group of cloud height, Cloud amount(of second group) in eighths, Quality of second group of cloud amount, Cloud type (of second group) in code, Quality of second group of cloud type, Cloud height (of second group) in feet, Quality of second group of cloud height, Cloud amount (of third group) in eighths, Quality of third group of cloud amount, Cloud type (of third group) in code, Quality of third group of cloud type, Cloud height (of third group) in feet, Quality of third group of cloud height, Cloud amount (of fourth group) in eighths, Quality of fourth group of cloud amount, Cloud type (of fourth group) in code, Quality of fourth group of cloud type, Cloud height (of fourth group) in feet, Quality of fourth group of cloud height, Ceilometer cloud amount (of first group), Quality of first group of ceilometer cloud amount, Ceilometer cloud height (of first group) in feet, Quality of first group of ceilometer cloud height, Ceilometer cloud amount (of second group), Quality of second group of ceilometer cloud amount, Ceilometer cloud height (of second group) in feet, Quality of second group of ceilometer cloud height, Ceilometer cloud amount (of third group), Quality of third group of ceilometer cloud amount, Ceilometer cloud height (of third group) in feet, Quality of third group of ceilometer cloud height, Ceilometer sky clear flag, Mean sea level pressure in hPa, Quality of mean sea level pressure, Station level pressure in hPa, Quality of station level pressure, AWS Flag, #\n</code></pre> <p>And then the data for each 30-minute interval. </p>"},{"location":"bom/#appendix-b-data-processing","title":"Appendix B: Data Processing","text":""},{"location":"bom/#1-station-details","title":"1. Station Details","text":"<p>We rename the fields to the following: </p> <ul> <li> <p>record_identifier </p> </li> <li> <p>station_number </p> </li> <li> <p>rainfall_district_code </p> </li> <li> <p>station_name </p> </li> <li> <p>month_year_station_opened </p> </li> <li> <p>month_year_station_closed </p> </li> <li> <p>latitude </p> </li> <li> <p>longitude </p> </li> <li> <p>method_by_which_latitude_longitude_was_derived </p> </li> <li> <p>state </p> </li> <li> <p>height_of_station_above_mean_sea_level_metres </p> </li> <li> <p>height_of_barometer_above_mean_sea_level_metres </p> </li> <li> <p>wmo_index_number, first_year_of_data_supplied </p> </li> <li> <p>last_year_of_data_supplied </p> </li> <li> <p>percentage_complete_between_first_and_last_records </p> </li> <li> <p>percentage_of_values_with_quality_flag_y </p> </li> <li> <p>percentage_of_values_with_quality_flag_n </p> </li> <li> <p>percentage_of_values_with_quality_flag_w </p> </li> <li> <p>percentage_of_values_with_quality_flag_s </p> </li> <li> <p>percentage_of_values_with_quality_flag_i </p> </li> <li> <p>end_of_record_indicator </p> </li> </ul> <p>Then we add a date_data_supplied column with value being the last day of the month for which the data is provided. Occasionally the data needs correction, for example state is missing we find and fill it in. </p>"},{"location":"bom/#2-weather-data","title":"2. Weather Data","text":"<p>We rename columns as follows: </p> <ul> <li> <p>\"Year Month Day Hour Minutes in YYYY\": \"Year\" </p> </li> <li> <p>\"MM\": \"Month\" </p> </li> <li> <p>\"DD\": \"Day\" </p> </li> <li> <p>\"HH24\": \"Hour Local STD\" </p> </li> <li> <p>\"MI format in Local standard time\": \"Min Local STD\" </p> </li> <li> <p>\"Relative humidity in percentage %\": \"Relative humidity in percentage\" </p> </li> <li> <p>\"#\": \"end of record indicator\" </p> </li> </ul> <p>and rename all the fields by replacing each white space with an underscore. </p> <p>Then change the data types (from string to integer or float) and treat the white spaces as Null according to the data type they should be. </p> <p>Integer columns: </p> <ul> <li> <p>station_number </p> </li> <li> <p>year </p> </li> <li> <p>month </p> </li> <li> <p>day </p> </li> <li> <p>hour_local_std </p> </li> <li> <p>min_local_std </p> </li> <li> <p>relative_humidity_in_percentage </p> </li> <li> <p>wind_direction_in_degrees_true </p> </li> <li> <p>cloud_height_of_first_group_in_feet </p> </li> <li> <p>cloud_height_of_second_group_in_feet </p> </li> <li> <p>cloud_height_of_third_group_in_feet </p> </li> <li> <p>cloud_height_of_fourth_group_in_feet </p> </li> <li> <p>ceilometer_cloud_height_of_first_group_in_feet </p> </li> <li> <p>ceilometer_cloud_height_of_second_group_in_feet </p> </li> <li> <p>ceilometer_cloud_height_of_third_group_in_feet </p> </li> <li> <p>ceilometer_sky_clear_flag, aws_flag </p> </li> </ul> <p>Float columns: </p> <ul> <li> <p>precipitation_since_9am_local_time_in_mm </p> </li> <li> <p>air_temperature_in_degrees_c </p> </li> <li> <p>wet_bulb_temperature_in_degrees_c </p> </li> <li> <p>dew_point_temperature_in_degrees_c </p> </li> <li> <p>vapour_pressure_in_hpa </p> </li> <li> <p>saturated_vapour_pressure_in_hpa </p> </li> <li> <p>wind_speed_in_km_h </p> </li> <li> <p>speed_of_maximum_windgust_in_last_10_minutes_in_km_h </p> </li> <li> <p>mean_sea_level_pressure_in_hpa </p> </li> <li> <p>station_level_pressure_in_hpa </p> </li> </ul> <p>Then we create datetime from the year, month, day, hour, minute values (local time) and add it as a new column. We find the state by looking up the station number in the station details and add state as a new column to the data.  </p>"},{"location":"edp/","title":"EDP (Energy Data Platform)","text":""},{"location":"edp/#database","title":"Database","text":"<p>The Energy Data Platform (EDP) is a comprehensive, research-grade dataset of high-resolution (5-minute) energy data from 951 Australian sites, spanning 2018\u20132025. It integrates:</p> <ul> <li> <p>Energy measurements: solar PV generation, household consumption, battery charging/discharging, voltage, and current.</p> </li> <li> <p>Site metadata: system specifications, inverter/subarray details, battery capacity, and connection type.</p> </li> <li> <p>Survey responses: household characteristics, occupancy, appliances, heating/cooling, and energy-related behaviors.</p> </li> </ul>"},{"location":"edp/#key-research-applications","title":"Key Research Applications","text":"<ol> <li> <p>Energy Behavior Analysis</p> <ul> <li> <p>Study consumption patterns, appliance usage, and load profiles.</p> </li> <li> <p>Investigate temporal trends, daily routines, and peak demand behavior.</p> </li> </ul> </li> <li> <p>Distributed Energy Resources (DER) Research</p> <ul> <li> <p>Evaluate solar PV generation and battery storage performance.</p> </li> <li> <p>Model and optimize energy systems at household or community scale.</p> </li> </ul> </li> <li> <p>Socio-Technical Insights</p> <ul> <li> <p>Link energy behavior to household demographics, dwelling type, and occupancy.</p> </li> <li> <p>Explore the influence of lifestyle and building characteristics on energy use.</p> </li> </ul> </li> </ol>"},{"location":"edp/#structure-data","title":"Structure &amp; Data","text":"<p>It consists of 5-minute energy data for 951 sites, spanning from 2018 to the end of June 2025. The database includes four types of tables, with one set being monthly partitions of all the data.</p> Database Table Columns edp_data_{year}_{month} edp_site_id, unix_time, datetime, edp_device_and_circuit, circuit_label, edp_circuit_label, real_energy, real_energy_negative, real_energy_positive, current_avg, current_min, current_max, voltage_avg, voltage_min, voltage_max <p>The data is provided by Solar Analytics and Wattwatchers and de-identified by UNSW. Solar Analytics site IDs start with \u201cS\u201d, while Wattwatchers site IDs start with \u201cW\u201d.</p> <p>Due to the large volume of time-series data, energy tables are partitioned by month (except for edp_data_2018). From 2019 onwards, data is stored in monthly partitions such as edp_data_2019_01 for January 2019, with each partition ranging between 1\u20132 GB (2018 has ~4 GB). Partitions are accessible via the data downloader as regular tables.</p> <p>Each site has different temporal coverage. For the latest updates on first and last available dates, refer to the tableedp_data_first_and_last_dates</p> Database Table Columns edp_data_first_and_last_dates edp_site_id, date_of_first_data, date_of_last_data edp_sites_metadata first_date_metadata_received, last_date_metadata_received, edp_site_id, state, site_time_zone, monitoring_hardware, inverter_manufacturer, inverter_model, subarray_manufacturer, subarray_model, inverter_ac_rating_kw, subarray_orientation, subarray_tilt, subarray_strings, subarray_modules_per_string, subarray_dc_rating_kw, islandable, has_battery, battery_size_make_model, limit_enabled, limit_amount, limit_applied edp_survey_answers edp_site_id, survey_date, consent_edp, consent_spt, consent_contact, consent_contact_method, over_18, postcode, state, account_holder, property_use, property_year, property_construction, property_construction_other, property_star_rating, num_floors, dwelling_type, dwelling_type_other, num_bedrooms, tenure, tenure_not_occupied, tenure_other, tenure_rented_from, tenure_rented_other, num_occupants, num_children, num_occupants_70plus, has_pets, num_type_pets, has_livestock, num_type_livestock, num_days_occupied, income_weekly, has_gas, has_gas_heating, has_gas_cooking, has_gas_hot_water, has_aircon, aircon_type, num_rooms_aircon, num_rooms_heated, num_refrigerators, has_pool_pump, dryer_usage, has_ev, ev_type, ev_charger_type, significant_loads_other, commercial_loads, business_hours, connection_type, has_controlled_load, islandable, property_power_outage_types, property_power_outage_other, area_power_outage_types, area_power_outage_other, power_outage_latest, hot_water_heat_type, hot_water_heat_type_other, rooms_heat_type, rooms_heat_type_other, has_battery, battery_size_make_model"},{"location":"edp/#appendix-a-original-data-and-preparation","title":"Appendix A: Original Data and Preparation","text":"<p>EDP consists of 5-minute energy data from Solar Analytics and Wattwatchers. Both provide energy data through their APIs and send us the survey responses and some metadata as excel files. </p>"},{"location":"edp/#1-solar-analytics","title":"1. Solar Analytics","text":"<p>The first batch of Solar Analytics customers that participate in the EDP project includes 129 sites from all around the country. The first survey for this group was performed in December 2021. The second batch we received data for completed the survey in April 2022 (a few in June 2022) and included 485 sites. For each batch, Solar Analytics has provided us with two excel files, one the survey responses, and one the system details (sites metadata). A few sites from each batch opted out soon after the project commenced, leaving us with a total of 599 sites. </p>"},{"location":"edp/#11-survey-responses","title":"1.1. Survey Responses","text":"<p>The survey questions and the answers for all sites were provided in an excel file. Complete questions were the headers of the columns and answers for each site were provided as a row. We have mapped each survey question to a column name. Below is the list of column mapping used for the survey questions. The actual questions and the answer options for each can be found on the Column Mapping spreadsheet. </p> # Field Name # Field Name # Field Name 1 CONSENT_EDP 23 DWELLING_TYPE 45 AIRCON_TYPE 2 CONSENT_SOLA 24 DWELLING_TYPE_OTHER 46 NUM_ROOMS_AIRCON 3 CONSENT_SPT 25 NUM_BEDROOMS 47 NUM_ROOMS_HEATED 4 CONSENT_CONTACT 26 TENURE 48 NUM_REFRIGERATORS 5 CONSENT_CONTACT_METHOD 27 TENURE_NOT_OCCUPIED 49 HAS_POOLPUMP 6 NAME 28 TENURE_OTHER 50 DRYER_USAGE 7 EMAIL 29 TENURE_RENTED_FROM 51 HAS_EV 8 PHONE 30 TENURE_RENTED_OTHER 52 EV_TYPE 9 OVER_18 31 NUM_OCCUMPANTS 53 EV_CHARGER_TYPE 10 SOLA_SITE_ID 32 NUM_CHILDREN 54 SIGNIFICANT_LOADS_OTHER 11 SOLA_SITE_NAME 33 NUM_OCCUPANTS_70PLUS 55 COMMERCIAL_LOADS 12 STREET 34 HAS_PETS 56 BUSINESS_HOURS 13 SUBURB 35 NUM_TYPE_PETS 57 CONNECTION_TYPE 14 POSTCODE 36 HAS_LIVESTOCK 58 HAS_CONTROLLED_LOAD 15 STATE 37 NUM_TYPE_LIVESTOCK 59 ISLANDABLE 16 ACCOUNT_HOLDER 38 NUM_DAYS_OCCUPIED 60 PROPERTY_POWER_OUTAGE_TYPES 17 PROPERTY_USE 39 INCOME_WEEKLY 61 PROPERTY_POWER_OUTAGE_OTHER 18 PROPERTY_YEAR 40 HAS_GAS 62 AREA_POWER_OUTAGE_TYPES 19 PROPERTY_CONSTRUCTION 41 HAS_GAS_HEATING 63 AREA_POWER_OUTAGE_OTHER 20 PROPERTY_CONSTRUCTION_OTHER 42 HAS_GAS_COOKING 64 POWER_OUTAGE_LATEST 21 PROPERTY_STAR_RATING 43 HAS_GAS_HOT_WATER 22 NUM_FLOORS 44 HAS_AIRCON <p>Questions 31 to 38 were asked twice; the second set (all having if known) had no answers, hence was not included. </p>"},{"location":"edp/#12-systems-metadata","title":"1.2. Systems Metadata","text":"<p>Solar Analytics has provided us with a second excel sheet containing the sites\u2019 metadata. The list of the column names is as follows: </p> # Field Name 1 Site ID 2 State 3 Monitoring Hardware 4 Inverter Manufacturer 5 Inverter Model 6 Inverter AC Rating (kW) 7 Subarray Manufacturer 8 Subarray Model 9 Subarray Orientation 10 Subarray Tilt 11 Subarray Strings 12 Subarray Modules per String 13 Subarray DC Rating (kW) 14 Has Battery 15 Opted in 16 Survey email <p>Each site has had one or more rows in this file. The data was mostly clean, most answers to each question in a unified format, number values in numeric format, etc. </p>"},{"location":"edp/#13-energy-data","title":"1.3. Energy Data","text":"<p>Energy timeseries data with the granularity of 5 minutes has been retrieved from Solar Analytic API. Then for each circuit we extract device name, circuit label, and data which includes the following fields: </p> <ul> <li> <p>time </p> </li> <li> <p>site id </p> </li> <li> <p>energy </p> </li> <li> <p>negative energy </p> </li> <li> <p>positive energy </p> </li> <li> <p>reactive energy </p> </li> <li> <p>power </p> </li> <li> <p>power factor </p> </li> <li> <p>reactive power </p> </li> <li> <p>apparent power </p> </li> <li> <p>voltage </p> </li> <li> <p>current </p> </li> </ul> <p>The returned timestamp values are in Unix format in site\u2019s local time. We convert the time to datetime format and keep both the original Unix time and the corresponding datetime. Also renamed the fields as below: </p> # Field Name 1 SolA Field Name 2 EDP Field Name 3 energy 4 realEnergy 5 energyNeg 6 realEnergyNegative 7 energyPos 8 realEnergyPositive 9 power 10 realPower"},{"location":"edp/#14-deidentification","title":"1.4. Deidentification","text":"<p>We de-identify all the sites and energy data. Site data includes site metadata and survey. We de-identify site data by removing name, full address, email, phone, and Solar Analytics customer ID. From the address fields we only keep postcode and state as they are required for some analyses but by themselves are not identifying. To de-identify the energy data we remove site ID and device names. For each site we generate and allocate a unique EDP site ID. This new id is then attached to all the data of the site, i.e., site metadata, survey, and energy timeseries data. Since we remove device name to deidentify the energy data, for each device we allocate a letter which we concatenate at the end of circuit label and add channel number after that. For example, if in the original data we have (typical for Solar Analytics energy data): </p> Site ID Device Name Circuit Label 98765 D111122233_1 ac_net_load <p>After the process of deidentification we will have: </p> edp_site_id edp_device_and_circuit edp_circuit_label S1234 A1 ac_net_load_A1 <p>In this example, device name D111122233 is replaced with letter A and together with the channel number becomes A1, is named edp_device_and_circuit, and also added to the circuit label to differentiate it from the data of other devices/circuits (becoming edp_circuit_label). </p>"},{"location":"edp/#2-wattwatchers","title":"2. Wattwatchers","text":"<p>We have been receiving data and metadata from 352 Wattwatchers sites, each site having 6 devices. This section will be completed with data extraction and de-identification processes, however, the major differences between data from Solar Analytics and Wattwatchers are explained in the next section. </p>"},{"location":"edp/#3-unifying-data-from-solar-analytics-and-wattwatchers-sites","title":"3. Unifying Data from Solar Analytics and Wattwatchers Sites","text":"<p>The aim has been to have the data from Solar Analytics and Wattwatchers in the same database tables: one table for all the energy data, one table for metadata, and one table for survey responses. </p>"},{"location":"edp/#31-unifying-data-fields","title":"3.1. Unifying Data Fields","text":""},{"location":"edp/#energy-data","title":"Energy data","text":"<p>There were no power columns in Wattwatchers data. Additionally, power fields (real power, apparent power, reactive power, power factor) are calculated from other fields and do not add any new information. Therefore, it was decided not to have those fields in the database, that is not to upload power fields of the Solar Analytics energy timeseries to the database. However, the deidentified csv files still include those fields. Additionally, Wattwatchers data had minimum and maximum voltage and current, while Solar Analytics data had average voltage and average current. It was decided to have three fields for each measure (min, max, average), fill the ones that have available values and keep the rest empty. </p>"},{"location":"edp/#metadata-and-survey","title":"Metadata and survey","text":"<p>To be completed. </p>"},{"location":"edp/#32-unifying-energy-units","title":"3.2. Unifying Energy Units","text":"<p>The energy data units were different for the two companies. Solar Analytics energy data unit was Wh while Wattwatchers energy data was in Joules. Therefore, we convert energy values of Wattwatchers from Joules to Wh. </p>"},{"location":"edp/#33-unifying-time","title":"3.3. Unifying Time","text":"<p>The Solar Analytics data is in local time but as UTC. We have converted it to local date and time and stored both values in the database (as timestamp and datetime). According to Solar Analytics because it is in local time, daylight saving is applied. The data for the one hour that the clocks are wound back is lost at Solar Analytics\u2019 end and currently there is not a way to get that data. Because it happens overnight though, we would expect no production and (they expect) very little constant consumption. For analysis, the data could probably be interpolated to re-create that hour if needed (or just be ignored if appropriate). Wattwatchers data comes as Unix timestamps in local time. We store timezone of each site in the metadata and survey tables. </p>"},{"location":"edp/#sola","title":"SolA","text":"Timestamp Date &amp; Time 1648950900 3/04/2022 1:55 1648951200 3/04/2022 2:00 1648951500 3/04/2022 2:05 ... ... 1648954200 3/04/2022 2:50 1648954500 3/04/2022 2:55 1648954800 3/04/2022 3:00 1648955100 3/04/2022 3:05"},{"location":"edp/#ww","title":"WW","text":"Timestamp Date &amp; Time 1648911300 3/04/2022 1:55 1648911600 3/04/2022 2:00 1648911900 3/04/2022 2:05 ... ... 1648914600 3/04/2022 2:50 1648914900 3/04/2022 2:55 1648915200 3/04/2022 2:00 1648915500 3/04/2022 2:05 1648915800 3/04/2022 2:10 ... ... 1648918500 3/04/2022 2:55 1648918800 3/04/2022 3:00 1648919100 3/04/2022 3:05"},{"location":"edp/#sola_1","title":"SolA","text":"Timestamp Date &amp; Time 1664675700 2/10/2022 1:55 1664676000 2/10/2022 2:00 1664676300 2/10/2022 2:05 ... ... 1664679000 2/10/2022 2:50 1664679300 2/10/2022 2:55 1664679600 2/10/2022 3:00 1664679900 2/10/2022 3:05"},{"location":"edp/#ww_1","title":"WW","text":"Timestamp Date &amp; Time 1664639700 2/10/2022 1:55 1664640000 2/10/2022 3:00 1664640300 2/10/2022 3:05"},{"location":"overview/","title":"Overview","text":""},{"location":"overview/#introduction","title":"Introduction","text":"<p>Data resource time-series hub (DARTH) is a growing database system of the School of Photovoltaic and Renewable Energy Engineering (SPREE). It consists of a cloud-based database and a web interface to access and download the data, available to all UNSW members (anyone with a zID). Direct access to the database can be granted as well. Please see the Access section for details.  </p> <p>This documentation will be further developed as the datasets grow and new ones are added, and in response to user feedback.</p>"},{"location":"overview/#data-sources","title":"Data Sources","text":"<p>The data in the DARTH database has two different types of sources:</p>"},{"location":"overview/#1-automatically-collected-data","title":"1. Automatically Collected Data","text":"<p>There is photovoltaics and weather data that has been automatically collected from sensors on UNSW Sydney buildings, processed, and uploaded to the database. The data is stored in the following tables:</p> <ul> <li>photovoltaics </li> <li>metadatapv </li> <li>metadataunitspv </li> <li>units </li> <li>versioninfo </li> <li>weather </li> <li>metadataunitsweather</li> </ul>"},{"location":"overview/#2-external-data-sources","title":"2. External Data Sources","text":"<p>Various datasets have been obtained from external sources, processed, and uploaded to the database. They include:</p> <ul> <li>Ausgrid 300 Homes Solar Data  </li> <li>Solar Analytics Solar Data  </li> <li>PV Bushfire Data  </li> <li>Victorian Consumers Data  </li> <li>EDP (Energy Data Platform)  </li> <li>BOM (weather data from the Australian Bureau of Meteorology)</li> </ul>"},{"location":"pv_bushfire/","title":"PV Bushfire Data","text":"<p>This data set ranges from 2017-11 to 2020-09 in 30-minute sampling rate. The data set includes 710 residential sites across Australia. </p>"},{"location":"pv_bushfire/#database-stricture-data","title":"Database Stricture &amp; Data","text":"<p>Two tables were created for this dataset: </p> <ol> <li> <p>pv_bushfire_site_details: contains the site details similar to the csv file, hence the following columns: </p> <ul> <li> <p>site_id (big integer) </p> </li> <li> <p>postcode (character (4)) </p> </li> <li> <p>state (character varying (3)) </p> </li> <li> <p>timezone_id (character varying) </p> </li> </ul> </li> <li> <p>pv_bushfire_data: contains all the PV energy data from all sites and times. The original data was in UTC; we localized timestamps using the time zone of each site and added a new column, datetime, to store the local time. It is referred to when accessing and filtering data via the webserver. The table has the following columns: </p> <ul> <li> <p>utc_timestamp (timestamp with time zone) </p> </li> <li> <p>datetime (timestamp without time zone) </p> </li> <li> <p>site_id (big integer) </p> </li> <li> <p>monitoring_data_type (character varying) </p> </li> <li> <p>value (real) </p> </li> </ul> </li> </ol> <p>The volume of this dataset is 2.7 GB (one month energy around 100 MB). </p>"},{"location":"pv_bushfire/#appendix-a-original-dataset","title":"Appendix A: Original Dataset","text":"<p>This data set ranges from 2017-11 to 2020-09 in 30-minute sampling rate.  </p> <p>The data set includes 710 residential sites across Australia from the Solar Analytics data set. </p> <p>Each month's data was provided in a separate csv file. There were 35 data files in total. In the data csv files, each column represents: </p> <ul> <li> <p>utc_timestamp: the time stamp in UTC </p> </li> <li> <p>site_id: the ID of this site </p> </li> <li> <p>monitoring_data_type: the data type of this monitoring </p> </li> <li> <p>values: the PV energy produced during the 30 minutes in w- att-hour </p> </li> </ul> <p>There was a site_details csv file as well, each row representing the details of one site as follows: </p> <ul> <li> <p>site_id: the ID of this site </p> </li> <li> <p>postcode: the postcode of this site </p> </li> <li> <p>state: the state of this site </p> </li> <li> <p>timezone_id: the local time zone of this site. This information will be useful when converting the UTC time stamp to local time stamp. </p> </li> </ul>"},{"location":"solar_analytics_2019/","title":"Solar Analytics Solar Data 2019 Dataset","text":""},{"location":"solar_analytics_2019/#database-structure-data","title":"Database Structure &amp; Data","text":"<p>This dataset has 5-minute energy data of 1000 sites for 1 year (2019). </p> <p>For this dataset two tables were created in the DARTH database: </p> <ol> <li> <p>solar_analytics_site_details with the following columns: </p> <ul> <li> <p>site_id (bigint) </p> </li> <li> <p>postcode (character 4) </p> </li> <li> <p>state (character varying 3) </p> </li> <li> <p>timezone_id (text) </p> </li> </ul> <p>The primary key is site_id. This table currently has 1000 records (40 KB). </p> </li> <li> <p>solar_analytics_voltage_data with these columns: </p> <ul> <li> <p>datetime (timestamp with time zone) </p> </li> <li> <p>site_id (bigint) </p> </li> <li> <p>energy_wh (double precision) </p> </li> <li> <p>voltage_max_v (double precision) </p> </li> <li> <p>voltage_min_v (double precision) </p> </li> </ul> </li> </ol> <p>This table currently has 105,406,509 records. The volume is 6 GB (each month\u2019s solar data around 500 MB). </p>"},{"location":"solar_analytics_2019/#appendix-a-original-dataset","title":"Appendix A: Original Dataset","text":"<p>The dataset included 12 csv files (2019-01 to 2019-12) and a site_details file. </p> <p>The site_details file included 1000 rows of site_id, postcode, state, and timezone_id. No missing data. </p> <p>Each csv file included around 9,000,000 rows of data with the following columns: t_stamp_utc, site_id, energy_(Wh), voltage_max_(V), voltage_min_(V). There was a small portion of records with missing data (Null values) for energy_(Wh), voltage_max_(V), or voltage_min_(V). No missing data for site_id. </p> <p>In total 1000 unique site_id values exist in site_details file and also 1000 unique site_id values exist in each csv file; however, some of the site_id values that have associated voltage data did not exist in the site_details file. That is, we have some voltage data for some (according to site_details file) unknown sites. The number of these unknown sites for each data file are 153 (for 10 csv files/months) and 164 (for 2 csv files/months). Additionally, some of the sites do not have any associated voltage data. </p>"},{"location":"tables_summary/","title":"Summary of Data Tables Accessible from the Web Downloader","text":""},{"location":"tables_summary/#datasets-accessible-via-the-spree-web-downloader","title":"Datasets Accessible via the SPREE Web Downloader","text":"<p>The following datasets are accessible via the SPREE web downloader. Please note that the volumes are as of September 2025; the data upload to the BOM and EDP datasets and photovoltaics and weather tables is continuous, so the dataset volumes constantly increase.</p>"},{"location":"tables_summary/#photovoltaics","title":"photovoltaics","text":"Tables (Data Type in Web Interface) Data Data Source Filters in Web Interface Volume photovoltaics, metadatapv 5-min PV data from 2018 to present UNSW buildings: PV-TETB, PV-MB, PV-UNIGYM, PV-TETB-ISLAND From Time Stamp, To Time Stamp, Building, Serial Number 1 GB"},{"location":"tables_summary/#weather","title":"weather","text":"Tables (Data Type in Web Interface) Data Data Source Filters in Web Interface Volume weather, metadataunitsweather Hourly weather data from 2020 to present UNSW buildings: TETB, Room401, TETB_SDEC From Time Stamp, To Time Stamp, Building 1.8 GB"},{"location":"tables_summary/#ausgrid-300-homes-solar-data","title":"Ausgrid 300 Homes Solar Data","text":"Tables (Data Type in Web Interface) Data Data Source Filters in Web Interface Volume ausgrid_solar_homes_customers, ausgrid_solar_homes_electricity 30-min solar generation data 2010-2013 300 sites From Time Stamp, To Time Stamp 800 MB"},{"location":"tables_summary/#solar-analytics-solar-data","title":"Solar Analytics Solar Data","text":"Tables (Data Type in Web Interface) Data Data Source Filters in Web Interface Volume solar_analytics_voltage_data, solar_analytics_site_details 5-min solar data (energy, voltage) from 2019-01 to 2019-12 1000 sites From Time Stamp, To Time Stamp 6 GB (site details 40 KB, one month\u2019s solar data ~500 MB)"},{"location":"tables_summary/#victorian-consumers-data","title":"Victorian Consumers Data","text":"Tables (Data Type in Web Interface) Data Data Source Filters in Web Interface Volume victorian_consumers_site_details, victorian_consumers_electricity_data, victorian_consumers_subload_data 30-min consumption, generation, voltage data; 30-min subload data from 2016-12 to 2022-05 1146 sites From Time Stamp, To Time Stamp 4 GB (electricity 2 GB, subload 2 GB)"},{"location":"tables_summary/#pv-bushfire-data","title":"PV Bushfire Data","text":"Tables (Data Type in Web Interface) Data Data Source Filters in Web Interface Volume pv_bushfire_data 30-min PV energy data from 2017-11 to 2020-09 710 residential sites From Time Stamp, To Time Stamp 2.7 GB (one month ~100 MB)"},{"location":"tables_summary/#bom-australian-bureau-of-meteorology","title":"BOM (Australian Bureau of Meteorology)","text":"Tables (Data Type in Web Interface) Data Data Source Filters in Web Interface Volume bom_data_nsw, bom_data_qld, \u2026 30-min weather data from 1985 to present 718 stations From Time Stamp, To Time Stamp 35 GB (one zone ~5 GB on average)"},{"location":"tables_summary/#edp-energy-data-platform","title":"EDP (Energy Data Platform)","text":"Tables (Data Type in Web Interface) Data Data Source Filters in Web Interface Volume edp_data (partitioned by year/month), edp_survey_answers, edp_sites_metadata 5-min energy data from 2018 to present ~1000 sites From Time Stamp, To Time Stamp 124 GB (one month ~1.5 GB)"},{"location":"victorian_consumers/","title":"Victorian Consumers Data","text":""},{"location":"victorian_consumers/#database-structure-data","title":"Database Structure &amp; Data","text":"<p>This dataset includes 30-minute consumption, generation, voltage data, 30-minute subload data, and site details. </p> <p>For this dataset three tables were created in the DARTH database: </p> <ol> <li> <p>victorian_consumers_site_details</p> <ul> <li> <p>darth_site_id (character 5, NOT NULL) </p> </li> <li> <p>darth_circuit_id (character 5, NOT NULL) </p> </li> <li> <p>dnsp (character varying) </p> </li> <li> <p>postcode (character 4, NOT NULL) </p> </li> <li> <p>connection_name (character varying) </p> </li> <li> <p>dc_cap_w (integer) </p> </li> <li> <p>num_phase (small int, NOT NULL) </p> </li> </ul> <p>The victorian_consumers_site_details table has 4726 rows. </p> </li> <li> <p>victorian_consumers_electricity_data</p> <ul> <li> <p>darth_site_id (character 5, NOT NULL) </p> </li> <li> <p>datetime (timestamp without time zone, NOT NULL) </p> </li> <li> <p>pv_energy_wh (real) </p> </li> <li> <p>pv_reactive_energy_varh (real) </p> </li> <li> <p>load_energy_wh (real) </p> </li> <li> <p>load_reactive_energy_varh (real) </p> </li> <li> <p>voltage_max_v (real) </p> </li> <li> <p>voltage_min_v (real) </p> </li> </ul> <p>The victorian_consumers_electricity_data table has 27,613,165 data rows which consists of the consumption, generation, and voltage data for 1146 sites. The time ranges from 2016-12-15 00:30 to 2022-05-24 00:00, varying between different sites. </p> </li> <li> <p>victorian_consumers_subload_data</p> <ul> <li> <p>darth_site_id (character 5, NOT NULL) </p> </li> <li> <p>darth_circuit_id (character 5, NOT NULL) </p> </li> <li> <p>datetime (timestamp without time zone, NOT NULL) </p> </li> <li> <p>subload_wh (real) </p> </li> <li> <p>subload_reactive_varh (real) </p> </li> <li> <p>voltage_max_v (real) </p> </li> <li> <p>voltage_min_v (real) </p> </li> </ul> <p>The victorian_consumers_subload_data table has 25,978,057 data rows which consists of the subload data for 501 sites. The time ranges from 2016-12-15 00:30 to 2022-05-24 00:00, varying between different sites. </p> </li> </ol> <p>The total volume of this dataset is around 4 GB. If the whole table is downloaded, each of the subload and electricity data tables will be around 2 GB. The site details table is comparatively very small, around 220 KB. </p>"},{"location":"victorian_consumers/#appendix-a-original-dataset","title":"Appendix A: Original Dataset","text":"<p>This dataset includes 30-minute consumption, generation, voltage data, 30-minute subload data, and site details. The original dataset provided by Solar Analytics included one csv file for site details, and one folder consisting of eight sub-folders each including around 200 csv files of load and subload data. The following details about the original dataset were provided by Solar Analytics. </p> <p>Columns for 30-minute consumption/generation/voltage data:</p> <ul> <li> <p>timestamp: timestamp in local time, note this is the ending timestamp of a 30-minute period (e.g., 00:30 refers to 00:00 - 00:30) </p> </li> <li> <p>pv_energy_(Wh): 30-minute PV energy for a site </p> </li> <li> <p>pv_reactive_energy_(VARh): 30-minute reactive PV energy for a site </p> </li> <li> <p>load_energy_(Wh): 30-minute load energy for a site </p> </li> <li> <p>load_reactive_energy_(VARh): 30-minute reactive load energy for a site </p> </li> <li> <p>voltage_max_(V): maximum 5-second voltage sampled over the 30-minute period </p> </li> <li> <p>voltage_min_(V): minimum 5-second voltage sampled over the 30-minute period </p> </li> </ul> <p>Note as our power values are just average power values derived from energy values, they are not included here as they could be easily converted (e.g., 600 Wh = 1200 W for a given 30 min time period). </p> <p>Columns for 30-minute subload data:</p> <ul> <li> <p>subload_(Wh): 30-minute subload energy </p> </li> <li> <p>subload_reactive_(VARh): 30-minute reactive subload energy </p> </li> <li> <p>c_id: circuit id     </p> </li> </ul> <p>Other columns are identical to the load data as above.</p> <p>Columns for site details:</p> <ul> <li> <p>site_id: site ID    </p> </li> <li> <p>c_id: circuit id     </p> </li> <li> <p>dnsp      </p> </li> <li> <p>postcode             </p> </li> <li> <p>connection_name: circuit type (note load_ is a subload circuit)   <li> <p>dc_cap_w: PV DC size in W </p> </li> <li> <p>num_phase: number of phases </p> </li>"},{"location":"victorian_consumers/#appendix-b-data-processing","title":"Appendix B: Data Processing","text":"<p>This dataset was deidentified as described below. </p>"},{"location":"victorian_consumers/#creating-new-site-ids-and-circuit-ids","title":"Creating New Site IDs and Circuit IDs","text":"<p>Every site id and circuit id in the were replaced by a new 5-character id that was generated by our processing program. First, the site details file was read, all the unique site ids extracted, the first one matched to S0001, second one matched to S0002, etc. This process continued until all the sites were matched with a new 5-character site id. A site IDs lookup table was generated and stored. Similarly, circuit ids in the site details file were matched to new 5-character ids (C0001, C0002, etc.) and a circuit IDs lookup table was generated and stored. </p>"},{"location":"victorian_consumers/#deidentification-of-site-details","title":"Deidentification of Site Details","text":"<p>Second step included deidentification of the site details file. Using the above lookup tables, new columns were added for darth_site_id and darth_circuit_id which included the corresponding 5-character id that matched site id and circuit id in each line. Then the original columns for site id and circuit id were removed. This deidentified site details table was stored as a csv file with the following columns, ready to be uploaded to DARTH: </p> <ul> <li> <p>darth_site_id </p> </li> <li> <p>darth_circuit_id </p> </li> <li> <p>dnsp </p> </li> <li> <p>postcode </p> </li> <li> <p>connection_name </p> </li> <li> <p>dc_cap_w </p> </li> <li> <p>num_phase </p> </li> </ul>"},{"location":"victorian_consumers/#deidentification-of-electricity-data","title":"Deidentification of Electricity Data","text":"<p>Each subfolder in the original dataset contained both electricity (consumption/generation/voltage) data files and subload data files, however they were named differently: the subload data files had subload word in in the file name after the site id. We used this to differentiate between electricity data files and subload data files within the data processing program. </p> <p>Each file was loaded into a DataFrame. The site id only existed in the name of the file, it was extracted and noted in a list to keep track of the processed sites, then deidentified using the id lookup table that previously generated and the deidentified site id was inserted as a new column to the DataFrame. The columns of the dataset were changed in preparation for uploading to the database: timestamp was renamed to datetime, parentheses removed, capitalized units were changed to lower case. The resulting data was stored as a csv file. Therefore, each deidentified electricity data file ready to be uploaded to the database included the following columns: </p> <ul> <li> <p>darth_site_id </p> </li> <li> <p>datetime </p> </li> <li> <p>pv_energy_wh </p> </li> <li> <p>pv_reactive_energy_varh </p> </li> <li> <p>load_energy_wh </p> </li> <li> <p>load_reactive_energy_varh </p> </li> <li> <p>voltage_max_v </p> </li> <li> <p>voltage_min_v </p> </li> </ul> <p>In total there were 1146 electricity data files, initially spread between eight subfolders together with subload data. 1146 deidentified electricity data files were generated by performing the above process. After finishing the deidentification process the total number of files in the original electricity data files and deidentified electricity data files were compared together and to the number of unique site ids processed. The data and column labels of a few original data files were compared to their corresponding deidentified electricity data files to ensure the correctness of the deidentification procedure. </p>"},{"location":"victorian_consumers/#deidentification-of-subload-data","title":"Deidentification of Subload Data","text":"<p>With an almost similar process as electricity data deidentification, the subload data files were deidentified. They had an additional column including circuit id which was deidentified using the circuits ids lookup table generated at the first step. This process was performed for each row rather than once for each file because for some of the sites multiple circuits existed in the subload data file. Each deidentified subload data file ready to be uploaded consisted of data in the following columns: </p> <ul> <li> <p>darth_site_id </p> </li> <li> <p>darth_circuit_id </p> </li> <li> <p>datetime </p> </li> <li> <p>subload_wh </p> </li> <li> <p>subload_reactive_varh </p> </li> <li> <p>voltage_max_v </p> </li> <li> <p>voltage_min_v </p> </li> </ul> <p>In total 501 subload data files originally existed and the same number of deidentified subload data files were generated and stored. </p>"}]}